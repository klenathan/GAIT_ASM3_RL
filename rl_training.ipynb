{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Training Notebook\n",
    "## Arena Game AI Training with PPO and DQN\n",
    "\n",
    "This notebook provides an interactive environment for training and evaluating RL agents for the Arena game.\n",
    "\n",
    "### Features:\n",
    "- üéÆ Train PPO and DQN agents\n",
    "- üìä Real-time training visualization\n",
    "- üîß Configurable hyperparameters\n",
    "- üìà Performance metrics and analysis\n",
    "- üíæ Model checkpointing and loading\n",
    "- üéØ Transfer learning support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from IPython.display import display, clear_output, Image\n",
    "import time\n",
    "\n",
    "# Stable Baselines3\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    BaseCallback, CheckpointCallback, CallbackList, EvalCallback\n",
    ")\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "# Local imports\n",
    "from arena.wrapper.arena_env import ArenaEnv\n",
    "from arena.core.callbacks import PlottingCallback, StopTrainingOnMaxEpisodes\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TRAINING CONFIGURATION ====================\n",
    "\n",
    "CONFIG = {\n",
    "    # Algorithm Selection\n",
    "    'algorithm': 'ppo',  # Options: 'ppo', 'dqn'\n",
    "    \n",
    "    # Control Style\n",
    "    'control_style': 2,  # 1=Rot/Thrust, 2=Direct (Up/Down/Left/Right)\n",
    "    \n",
    "    # Training Parameters\n",
    "    'total_timesteps': 1_000_000,  # Total training steps\n",
    "    'max_episodes': None,  # Stop after X episodes (None = unlimited)\n",
    "    'n_envs': 8,  # Number of parallel environments\n",
    "    \n",
    "    # Checkpoint Settings\n",
    "    'checkpoint_freq': 50_000,  # Save model every X steps\n",
    "    'eval_freq': 10_000,  # Evaluate model every X steps\n",
    "    'n_eval_episodes': 10,  # Number of episodes for evaluation\n",
    "    \n",
    "    # Directories\n",
    "    'model_dir': 'models',\n",
    "    'log_dir': 'logs',\n",
    "    'tensorboard_log': 'logs/tensorboard',\n",
    "    \n",
    "    # Transfer Learning\n",
    "    'load_model': None,  # Path to pretrained model (None = train from scratch)\n",
    "    \n",
    "    # PPO Hyperparameters\n",
    "    'ppo': {\n",
    "        'learning_rate': 3e-4,\n",
    "        'n_steps': 2048,\n",
    "        'batch_size': 64,\n",
    "        'n_epochs': 10,\n",
    "        'gamma': 0.99,\n",
    "        'gae_lambda': 0.95,\n",
    "        'clip_range': 0.2,\n",
    "        'ent_coef': 0.01,  # Entropy coefficient for exploration\n",
    "        'vf_coef': 0.5,\n",
    "    },\n",
    "    \n",
    "    # DQN Hyperparameters\n",
    "    'dqn': {\n",
    "        'learning_rate': 1e-4,\n",
    "        'buffer_size': 100_000,\n",
    "        'batch_size': 32,\n",
    "        'gamma': 0.99,\n",
    "        'tau': 1.0,\n",
    "        'exploration_fraction': 0.1,\n",
    "        'exploration_initial_eps': 1.0,\n",
    "        'exploration_final_eps': 0.05,\n",
    "        'target_update_interval': 1000,\n",
    "        'train_freq': 4,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['model_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['log_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['tensorboard_log'], exist_ok=True)\n",
    "\n",
    "# Display configuration\n",
    "print(\"üéØ Training Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in CONFIG.items():\n",
    "    if not isinstance(value, dict):\n",
    "        print(f\"{key:25s}: {value}\")\n",
    "print()\n",
    "print(f\"üì¶ {CONFIG['algorithm'].upper()} Hyperparameters:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in CONFIG[CONFIG['algorithm']].items():\n",
    "    print(f\"{key:25s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Callback for Jupyter Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JupyterPlottingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for real-time plotting in Jupyter notebooks.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir: str, plot_freq: int = 5000, verbose=1):\n",
    "        super(JupyterPlottingCallback, self).__init__(verbose)\n",
    "        self.log_dir = log_dir\n",
    "        self.plot_freq = plot_freq\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.timesteps = []\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.plot_freq == 0:\n",
    "            self._plot_progress()\n",
    "        return True\n",
    "    \n",
    "    def _plot_progress(self):\n",
    "        try:\n",
    "            # Load monitor results\n",
    "            df = load_results(self.log_dir)\n",
    "            if len(df) < 2:\n",
    "                return\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Create comprehensive visualization\n",
    "            fig = plt.figure(figsize=(16, 10))\n",
    "            gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            # Calculate rolling statistics\n",
    "            window = min(100, max(1, len(df) // 20))\n",
    "            rewards = df['r'].values\n",
    "            lengths = df['l'].values\n",
    "            episodes = np.arange(len(df))\n",
    "            \n",
    "            rewards_mean = pd.Series(rewards).rolling(window=window, min_periods=1).mean()\n",
    "            lengths_mean = pd.Series(lengths).rolling(window=window, min_periods=1).mean()\n",
    "            \n",
    "            # Plot 1: Episode Rewards\n",
    "            ax1 = fig.add_subplot(gs[0, 0])\n",
    "            ax1.plot(episodes, rewards, alpha=0.2, color='steelblue', linewidth=0.5)\n",
    "            ax1.plot(episodes, rewards_mean, color='darkblue', linewidth=2, label=f'Rolling Mean ({window})')\n",
    "            ax1.fill_between(episodes, rewards, rewards_mean, alpha=0.1, color='steelblue')\n",
    "            ax1.set_xlabel('Episode', fontsize=10)\n",
    "            ax1.set_ylabel('Reward', fontsize=10)\n",
    "            ax1.set_title('üìä Episode Rewards', fontsize=12, fontweight='bold')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 2: Episode Lengths\n",
    "            ax2 = fig.add_subplot(gs[0, 1])\n",
    "            ax2.plot(episodes, lengths, alpha=0.2, color='forestgreen', linewidth=0.5)\n",
    "            ax2.plot(episodes, lengths_mean, color='darkgreen', linewidth=2, label=f'Rolling Mean ({window})')\n",
    "            ax2.fill_between(episodes, lengths, lengths_mean, alpha=0.1, color='forestgreen')\n",
    "            ax2.set_xlabel('Episode', fontsize=10)\n",
    "            ax2.set_ylabel('Length (steps)', fontsize=10)\n",
    "            ax2.set_title('‚è±Ô∏è Episode Lengths', fontsize=12, fontweight='bold')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 3: Reward Distribution\n",
    "            ax3 = fig.add_subplot(gs[1, 0])\n",
    "            recent_rewards = rewards[-min(500, len(rewards)):]\n",
    "            ax3.hist(recent_rewards, bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "            ax3.axvline(np.mean(recent_rewards), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(recent_rewards):.2f}')\n",
    "            ax3.axvline(np.median(recent_rewards), color='blue', linestyle='--', linewidth=2, label=f'Median: {np.median(recent_rewards):.2f}')\n",
    "            ax3.set_xlabel('Reward', fontsize=10)\n",
    "            ax3.set_ylabel('Frequency', fontsize=10)\n",
    "            ax3.set_title('üìà Recent Reward Distribution (Last 500 Episodes)', fontsize=12, fontweight='bold')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Plot 4: Success Rate Over Time\n",
    "            ax4 = fig.add_subplot(gs[1, 1])\n",
    "            success_threshold = 0  # Define what constitutes success\n",
    "            success = (rewards > success_threshold).astype(int)\n",
    "            success_rate = pd.Series(success).rolling(window=window, min_periods=1).mean() * 100\n",
    "            ax4.plot(episodes, success_rate, color='purple', linewidth=2)\n",
    "            ax4.fill_between(episodes, 0, success_rate, alpha=0.3, color='purple')\n",
    "            ax4.set_xlabel('Episode', fontsize=10)\n",
    "            ax4.set_ylabel('Success Rate (%)', fontsize=10)\n",
    "            ax4.set_title('üéØ Success Rate (Reward > 0)', fontsize=12, fontweight='bold')\n",
    "            ax4.set_ylim([0, 105])\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 5: Statistics Summary\n",
    "            ax5 = fig.add_subplot(gs[2, :])\n",
    "            ax5.axis('off')\n",
    "            \n",
    "            stats_text = f\"\"\"\n",
    "            üìä TRAINING STATISTICS\n",
    "            {'=' * 80}\n",
    "            \n",
    "            Total Episodes: {len(df):,} | Total Timesteps: {self.num_timesteps:,} / {CONFIG['total_timesteps']:,} ({100*self.num_timesteps/CONFIG['total_timesteps']:.1f}%)\n",
    "            \n",
    "            Recent Performance (Last {min(100, len(df))} Episodes):\n",
    "              ‚Ä¢ Mean Reward: {np.mean(rewards[-100:]):.2f} ¬± {np.std(rewards[-100:]):.2f}\n",
    "              ‚Ä¢ Best Reward: {np.max(rewards[-100:]):.2f}\n",
    "              ‚Ä¢ Mean Length: {np.mean(lengths[-100:]):.1f} steps\n",
    "              ‚Ä¢ Success Rate: {100*np.mean(rewards[-100:] > 0):.1f}%\n",
    "            \n",
    "            Overall Performance:\n",
    "              ‚Ä¢ Mean Reward: {np.mean(rewards):.2f} ¬± {np.std(rewards):.2f}\n",
    "              ‚Ä¢ Best Reward: {np.max(rewards):.2f}\n",
    "              ‚Ä¢ Worst Reward: {np.min(rewards):.2f}\n",
    "              ‚Ä¢ Mean Length: {np.mean(lengths):.1f} steps\n",
    "            \"\"\"\n",
    "            \n",
    "            ax5.text(0.1, 0.5, stats_text, fontsize=11, family='monospace',\n",
    "                    verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "            \n",
    "            plt.suptitle(f'üéÆ RL Training Progress - {CONFIG[\"algorithm\"].upper()} | Style {CONFIG[\"control_style\"]}',\n",
    "                        fontsize=14, fontweight='bold', y=0.995)\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error plotting: {e}\")\n",
    "\n",
    "print(\"‚úÖ Custom callback defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorized environment\n",
    "print(f\"üéÆ Creating {CONFIG['n_envs']} parallel environments...\")\n",
    "\n",
    "env = make_vec_env(\n",
    "    lambda: ArenaEnv(control_style=CONFIG['control_style']),\n",
    "    n_envs=CONFIG['n_envs'],\n",
    "    monitor_dir=CONFIG['log_dir']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Environment created!\")\n",
    "print(f\"   - Observation Space: {env.observation_space}\")\n",
    "print(f\"   - Action Space: {env.action_space}\")\n",
    "print(f\"   - Control Style: {'Rot/Thrust' if CONFIG['control_style'] == 1 else 'Direct'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"{CONFIG['algorithm']}_style{CONFIG['control_style']}\"\n",
    "\n",
    "if CONFIG['load_model']:\n",
    "    # Load pretrained model\n",
    "    load_path = CONFIG['load_model']\n",
    "    if load_path.endswith('.zip'):\n",
    "        load_path = load_path[:-4]\n",
    "    \n",
    "    print(f\"üì• Loading model from {load_path}...\")\n",
    "    \n",
    "    if CONFIG['algorithm'] == 'ppo':\n",
    "        model = PPO.load(load_path, env=env, verbose=1, tensorboard_log=CONFIG['tensorboard_log'])\n",
    "    else:\n",
    "        model = DQN.load(load_path, env=env, verbose=1, tensorboard_log=CONFIG['tensorboard_log'])\n",
    "    \n",
    "    reset_timesteps = False\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    \n",
    "else:\n",
    "    # Create new model\n",
    "    print(f\"üÜï Creating new {CONFIG['algorithm'].upper()} model...\")\n",
    "    \n",
    "    if CONFIG['algorithm'] == 'ppo':\n",
    "        model = PPO(\n",
    "            \"MlpPolicy\",\n",
    "            env,\n",
    "            verbose=1,\n",
    "            tensorboard_log=CONFIG['tensorboard_log'],\n",
    "            **CONFIG['ppo']\n",
    "        )\n",
    "    else:\n",
    "        model = DQN(\n",
    "            \"MlpPolicy\",\n",
    "            env,\n",
    "            verbose=1,\n",
    "            tensorboard_log=CONFIG['tensorboard_log'],\n",
    "            **CONFIG['dqn']\n",
    "        )\n",
    "    \n",
    "    reset_timesteps = True\n",
    "    print(\"‚úÖ Model created successfully!\")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nüìê Model Architecture:\")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=CONFIG['checkpoint_freq'] // CONFIG['n_envs'],\n",
    "    save_path=CONFIG['model_dir'],\n",
    "    name_prefix=model_name\n",
    ")\n",
    "\n",
    "# Jupyter plotting callback\n",
    "jupyter_plotting_callback = JupyterPlottingCallback(\n",
    "    log_dir=CONFIG['log_dir'],\n",
    "    plot_freq=1000  # Update plot every 1000 steps\n",
    ")\n",
    "\n",
    "# File plotting callback (for saved graphs)\n",
    "file_plotting_callback = PlottingCallback(\n",
    "    log_dir=CONFIG['log_dir'],\n",
    "    plot_freq=5000\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint_callback, jupyter_plotting_callback, file_plotting_callback]\n",
    "\n",
    "# Optional: Stop on max episodes\n",
    "if CONFIG['max_episodes'] is not None:\n",
    "    stop_callback = StopTrainingOnMaxEpisodes(\n",
    "        max_episodes=CONFIG['max_episodes'],\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks_list.append(stop_callback)\n",
    "    print(f\"‚è±Ô∏è Training will stop after {CONFIG['max_episodes']} episodes\")\n",
    "\n",
    "callbacks = CallbackList(callbacks_list)\n",
    "\n",
    "print(\"‚úÖ Callbacks configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "üéØ **Training will begin when you run this cell.**\n",
    "\n",
    "Progress will be displayed above with real-time plots updating every few thousand steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üöÄ Starting training: {model_name}\")\n",
    "print(f\"   Total Timesteps: {CONFIG['total_timesteps']:,}\")\n",
    "print(f\"   Parallel Envs: {CONFIG['n_envs']}\")\n",
    "if CONFIG['max_episodes']:\n",
    "    print(f\"   Max Episodes: {CONFIG['max_episodes']}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    model.learn(\n",
    "        total_timesteps=CONFIG['total_timesteps'],\n",
    "        callback=callbacks,\n",
    "        reset_num_timesteps=reset_timesteps,\n",
    "        tb_log_name=model_name\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"‚è±Ô∏è Total training time: {training_time/60:.2f} minutes\")\n",
    "    print(f\"‚ö° Average: {CONFIG['total_timesteps']/training_time:.0f} steps/second\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è Training time: {training_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_model_path = os.path.join(CONFIG['model_dir'], f\"{model_name}_final_{timestamp}\")\n",
    "\n",
    "model.save(final_model_path)\n",
    "print(f\"üíæ Final model saved to: {final_model_path}.zip\")\n",
    "\n",
    "# Also save with a simple name for easy loading\n",
    "simple_path = os.path.join(CONFIG['model_dir'], f\"{model_name}_final\")\n",
    "model.save(simple_path)\n",
    "print(f\"üíæ Also saved as: {simple_path}.zip (for easy loading)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Evaluating trained model...\")\n",
    "\n",
    "# Create evaluation environment (single env)\n",
    "eval_env = ArenaEnv(control_style=CONFIG['control_style'])\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# Evaluate\n",
    "n_eval_episodes = 20\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=n_eval_episodes,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Evaluation Results ({n_eval_episodes} episodes):\")\n",
    "print(f\"   Mean Reward: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training results\n",
    "df = load_results(CONFIG['log_dir'])\n",
    "\n",
    "print(f\"üìà Training Summary:\")\n",
    "print(f\"   Total Episodes: {len(df):,}\")\n",
    "print(f\"   Mean Reward: {df['r'].mean():.2f} ¬± {df['r'].std():.2f}\")\n",
    "print(f\"   Best Reward: {df['r'].max():.2f}\")\n",
    "print(f\"   Worst Reward: {df['r'].min():.2f}\")\n",
    "print(f\"   Mean Episode Length: {df['l'].mean():.1f} steps\")\n",
    "\n",
    "# Display saved training graph\n",
    "graph_path = os.path.join(CONFIG['log_dir'], 'training_graph.png')\n",
    "if os.path.exists(graph_path):\n",
    "    print(f\"\\nüìä Training Graph:\")\n",
    "    display(Image(filename=graph_path))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training graph not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(f'Comprehensive Training Analysis - {model_name}', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Calculate statistics\n",
    "window = 100\n",
    "rewards = df['r'].values\n",
    "lengths = df['l'].values\n",
    "episodes = np.arange(len(df))\n",
    "\n",
    "rewards_rolling = pd.Series(rewards).rolling(window=window, min_periods=1).mean()\n",
    "lengths_rolling = pd.Series(lengths).rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "# 1. Rewards over time\n",
    "axes[0, 0].plot(episodes, rewards, alpha=0.3, label='Raw')\n",
    "axes[0, 0].plot(episodes, rewards_rolling, linewidth=2, label=f'Rolling Mean ({window})')\n",
    "axes[0, 0].set_title('Episode Rewards')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Episode lengths over time\n",
    "axes[0, 1].plot(episodes, lengths, alpha=0.3, label='Raw')\n",
    "axes[0, 1].plot(episodes, lengths_rolling, linewidth=2, label=f'Rolling Mean ({window})')\n",
    "axes[0, 1].set_title('Episode Lengths')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Steps')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Reward distribution\n",
    "axes[0, 2].hist(rewards, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 2].axvline(rewards.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {rewards.mean():.2f}')\n",
    "axes[0, 2].set_title('Reward Distribution')\n",
    "axes[0, 2].set_xlabel('Reward')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Learning curve (cumulative average)\n",
    "cumulative_avg = np.cumsum(rewards) / (episodes + 1)\n",
    "axes[1, 0].plot(episodes, cumulative_avg, linewidth=2)\n",
    "axes[1, 0].set_title('Cumulative Average Reward')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Average Reward')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Reward vs Episode Length\n",
    "axes[1, 1].scatter(lengths, rewards, alpha=0.3, s=10)\n",
    "axes[1, 1].set_title('Reward vs Episode Length')\n",
    "axes[1, 1].set_xlabel('Episode Length')\n",
    "axes[1, 1].set_ylabel('Reward')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Performance improvement\n",
    "# Split into chunks and show mean reward per chunk\n",
    "n_chunks = 10\n",
    "chunk_size = len(df) // n_chunks\n",
    "chunk_means = [rewards[i*chunk_size:(i+1)*chunk_size].mean() for i in range(n_chunks)]\n",
    "chunk_stds = [rewards[i*chunk_size:(i+1)*chunk_size].std() for i in range(n_chunks)]\n",
    "chunk_labels = [f\"{i*chunk_size}-{(i+1)*chunk_size}\" for i in range(n_chunks)]\n",
    "\n",
    "axes[1, 2].bar(range(n_chunks), chunk_means, yerr=chunk_stds, capsize=5, alpha=0.7)\n",
    "axes[1, 2].set_title('Performance by Training Chunk')\n",
    "axes[1, 2].set_xlabel('Episode Range')\n",
    "axes[1, 2].set_ylabel('Mean Reward')\n",
    "axes[1, 2].set_xticks(range(n_chunks))\n",
    "axes[1, 2].set_xticklabels([f\"#{i+1}\" for i in range(n_chunks)])\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save analysis\n",
    "analysis_path = os.path.join(CONFIG['log_dir'], f'{model_name}_analysis.png')\n",
    "fig.savefig(analysis_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nüíæ Analysis saved to: {analysis_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Trained Agent (Visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This will open a pygame window\n",
    "# You may need to run this in a regular Python environment rather than Jupyter for best results\n",
    "\n",
    "print(\"üéÆ Testing agent with visual rendering...\")\n",
    "print(\"   (Close the pygame window to stop)\\n\")\n",
    "\n",
    "test_env = ArenaEnv(control_style=CONFIG['control_style'], render_mode='human')\n",
    "\n",
    "obs, _ = test_env.reset()\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "done = False\n",
    "\n",
    "try:\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = test_env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        test_env.render()\n",
    "        \n",
    "    print(f\"\\nüìä Test Episode Results:\")\n",
    "    print(f\"   Reward: {episode_reward:.2f}\")\n",
    "    print(f\"   Length: {episode_length} steps\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Test interrupted\")\n",
    "    \n",
    "finally:\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Compare with Baseline (Optional)\n",
    "\n",
    "If you have multiple trained models, you can compare them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare PPO vs DQN, or Style 1 vs Style 2\n",
    "\n",
    "models_to_compare = [\n",
    "    # Add model paths here, e.g.:\n",
    "    # ('PPO Style 1', 'models/ppo_style1_final'),\n",
    "    # ('PPO Style 2', 'models/ppo_style2_final'),\n",
    "    # ('DQN Style 2', 'models/dqn_style2_final'),\n",
    "]\n",
    "\n",
    "if models_to_compare:\n",
    "    print(\"üîÑ Comparing models...\\n\")\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for model_label, model_path in models_to_compare:\n",
    "        # Determine algorithm from name\n",
    "        algo = 'ppo' if 'ppo' in model_path.lower() else 'dqn'\n",
    "        style = 1 if 'style1' in model_path.lower() else 2\n",
    "        \n",
    "        # Load model\n",
    "        if algo == 'ppo':\n",
    "            test_model = PPO.load(model_path)\n",
    "        else:\n",
    "            test_model = DQN.load(model_path)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_env = ArenaEnv(control_style=style)\n",
    "        test_env = Monitor(test_env)\n",
    "        \n",
    "        mean_r, std_r = evaluate_policy(test_model, test_env, n_eval_episodes=20)\n",
    "        \n",
    "        comparison_results.append({\n",
    "            'Model': model_label,\n",
    "            'Mean Reward': mean_r,\n",
    "            'Std Reward': std_r\n",
    "        })\n",
    "        \n",
    "        test_env.close()\n",
    "        print(f\"‚úÖ {model_label}: {mean_r:.2f} ¬± {std_r:.2f}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    df_comparison = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(df_comparison['Model'], df_comparison['Mean Reward'], \n",
    "            yerr=df_comparison['Std Reward'], capsize=10, alpha=0.7)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Mean Reward', fontsize=12)\n",
    "    plt.title('Model Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Comparison DataFrame:\")\n",
    "    display(df_comparison)\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No models specified for comparison\")\n",
    "    print(\"   Add model paths to 'models_to_compare' list above to compare models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training data to CSV for further analysis\n",
    "export_path = os.path.join(CONFIG['log_dir'], f'{model_name}_training_data.csv')\n",
    "df.to_csv(export_path, index=False)\n",
    "print(f\"üíæ Training data exported to: {export_path}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìä Training Data Preview:\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(f\"\\nüìà Data Shape: {df.shape}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüì¶ Model: {model_name}\")\n",
    "print(f\"üéÆ Algorithm: {CONFIG['algorithm'].upper()}\")\n",
    "print(f\"üïπÔ∏è Control Style: {CONFIG['control_style']} ({'Rot/Thrust' if CONFIG['control_style'] == 1 else 'Direct'})\")\n",
    "print(f\"\\nüìä Final Performance:\")\n",
    "print(f\"   Evaluation Reward: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "print(f\"   Training Episodes: {len(df):,}\")\n",
    "print(f\"   Total Timesteps: {CONFIG['total_timesteps']:,}\")\n",
    "\n",
    "print(f\"\\nüíæ Saved Files:\")\n",
    "print(f\"   - Model: {simple_path}.zip\")\n",
    "print(f\"   - Training Graph: {graph_path}\")\n",
    "print(f\"   - Training Data: {export_path}\")\n",
    "print(f\"   - Analysis: {analysis_path}\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Review training graphs and metrics above\")\n",
    "print(\"   2. Test the agent visually (Section 12)\")\n",
    "print(\"   3. Try different hyperparameters or control styles\")\n",
    "print(\"   4. Use transfer learning by setting 'load_model' in config\")\n",
    "print(\"   5. Compare different models (Section 13)\")\n",
    "print(\"   6. Launch TensorBoard: tensorboard --logdir=\" + CONFIG['tensorboard_log'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
