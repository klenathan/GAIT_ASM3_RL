# Arena RL: OVERFIT & GROKKING Training Strategy

## ðŸŽ¯ Philosophy: "Overfit to Win, Then Generalize"

Instead of trying to learn a general policy from the start, we use a **grokking recipe** that:
1. **Overfits completely** to the simplest possible scenario
2. **Masters ONE winning strategy** (even if narrow/exploitative)
3. **Gradually transfers** the learned behavior to more complex scenarios
4. **Groks the core mechanics** through intensive practice on atomic tasks

This is inspired by:
- **Grokking** in deep learning (sudden generalization after overfitting)
- **Go-Explore** (systematic exploration from promising states)
- **Curriculum Learning** (progressively harder tasks)
- **AlphaStar/OpenAI Five** (narrow task mastery â†’ transfer)

---

## ðŸš€ What Was Implemented

### **Grade 0: OVERFIT Single Spawner** (NEW!)

The **simplest possible task** designed for guaranteed learning.

#### Configuration:
```python
CurriculumStage(
    name="Grade 0: OVERFIT Single Spawner",
    
    # ENVIRONMENT
    fixed_spawner_positions=[(800, 640)],  # Always center of arena
    spawner_health_mult=0.15,              # 15 HP (only 1.5 shots!)
    max_episode_steps=5000,                # More time (vs 3000 default)
    
    # NO DISTRACTIONS
    spawn_cooldown_mult=999.0,             # No enemies spawn
    max_enemies_mult=0.0,                  # No enemies exist
    
    # MAXIMUM GUIDANCE
    shaping_scale_mult=15.0,               # 15Ã— all shaping rewards!
    damage_penalty_mult=0.2,               # Low penalty (encourage exploration)
    
    # ADVANCEMENT (Must Master It!)
    min_spawner_kill_rate=0.8,             # 80% kill rate required
    min_damage_dealt=10.0,                 # Just some damage
    min_episodes=200,                      # Extensive practice
)
```

#### What the Agent Learns:
1. **Approach** the center spawner (proximity reward: up to +0.75 per step!)
2. **Aim** at the spawner (aimed shot reward: up to +15.0 per shot!)
3. **Shoot** accurately (hit reward: +6.0 per hit)
4. **Kill** the spawner (**+400 MASSIVE reward!**)

#### Why It Works:
- **Fixed position**: Agent can memorize "go to center" pattern
- **15 HP spawner**: Only 1.5 shots needed (almost instant feedback!)
- **15Ã— shaping**: Every small correct action is STRONGLY rewarded
- **400 kill reward**: Makes winning **irresistible**
- **No enemies**: Agent can focus 100% on spawner mechanics
- **5000 steps**: Plenty of time to experiment and learn

---

## ðŸ“Š Curriculum Progression (9 Grades Total)

### **Grade 0: Overfit Single Spawner** (NEW!)
- **Goal**: Master killing ONE fixed spawner
- **Spawner**: 15 HP, center position (800, 640)
- **Enemies**: 0
- **Shaping**: 15Ã— (maximum)
- **Expected**: 80% kill rate within **1-2M steps** âœ“

### **Grade 1: Transfer to Random Positions**
- **Goal**: Transfer skill to random spawner positions
- **Spawner**: 30 HP, random positions
- **Enemies**: 0
- **Shaping**: 5Ã— (high)
- **Expected**: 0.3 kills/episode within 2-3M steps

### **Grades 2-8: Progressive Difficulty**
- **Grade 2**: Damage spawner consistently
- **Grade 3**: Kill 1 spawner per episode
- **Grade 4**: Kill multiple spawners
- **Grade 5**: Introduce weak enemies
- **Grade 6**: More enemies, faster kills
- **Grade 7**: High enemy density
- **Grade 8**: Full difficulty (elite performance)

---

## ðŸ”¥ Key Optimizations for Grokking

### 1. **10Ã— Spawner Kill Reward**
```python
# arena/core/config.py
REWARD_SPAWNER_DESTROYED = 400.0  # Was 40
```

**Impact**: Killing a spawner now gives **+400** (vs +40 before)
- Makes spawner kills **10Ã— more valuable** than before
- Agent will explore extensively to find this massive reward
- Creates strong value gradient toward spawner destruction

### 2. **15Ã— Shaping Rewards in Grade 0**
```python
shaping_scale_mult=15.0  # Maximum guidance
```

**All** shaping rewards scaled by 15Ã—:
- **Proximity reward**: Â±0.05 â†’ **Â±0.75** per step!
- **Aimed shot reward**: +1.0 â†’ **+15.0** per shot!
- **Combat efficiency**: +0.5 â†’ **+7.5**
- **Style 2 alignment**: +0.08 â†’ **+1.2**

**Impact**: Every correct micro-behavior gets MASSIVE positive feedback.

### 3. **Fixed Spawner Position**
```python
fixed_spawner_positions=[(800, 640)]  # Center of 1600Ã—1280 arena
```

**Impact**: 
- Agent can **memorize** the exact path to spawner
- Eliminates variance from random spawning
- Allows **deep overfitting** to ONE scenario
- Faster convergence (no need to generalize initially)

### 4. **Ultra-Weak Spawner (15 HP)**
```python
spawner_health_mult=0.15  # 15 HP (1.5 shots!)
```

**Impact**:
- Base spawner: 100 HP (10 shots)
- Grade 0 spawner: **15 HP (1.5 shots!)**
- Near-instant kill feedback
- Agent can kill spawner **accidentally** and learn from it

### 5. **Extended Episode Length (5000 steps)**
```python
max_episode_steps=5000  # vs 3000 default
```

**Impact**:
- Agent has **67% more time** to explore and learn
- Reduces pressure to find solution quickly
- More opportunities per episode to discover spawner kill

---

## ðŸ“ˆ Expected Training Timeline

### **Phase 1: Random Exploration (0-200K steps)**
- Agent moves randomly
- Occasionally approaches spawner (proximity reward guides)
- May accidentally shoot near spawner (aimed shot reward!)
- **Milestone**: First time moving toward spawner

### **Phase 2: Approach Behavior (200K-500K steps)**
- Agent learns approaching spawner gives high reward
- Starts consistently moving toward center
- **Milestone**: 50%+ episodes reach spawner

### **Phase 3: Shooting Discovery (500K-800K steps)**
- Agent discovers shooting gives reward
- Learns to shoot WHILE near spawner
- **Milestone**: First spawner damage dealt

### **Phase 4: Kill Mastery (800K-1.5M steps)**
- Agent discovers **+400 reward** from spawner kill!
- Policy rapidly improves to maximize kills
- **Milestone**: First spawner kill! ðŸŽ‰

### **Phase 5: Consistent Performance (1.5M-2M steps)**
- Agent achieves **80%+ kill rate**
- Policy becomes deterministic
- **Milestone**: Advance to Grade 1!

---

## ðŸ§ª Validation Test Results

```bash
python test_overfit_curriculum.py
```

### âœ… All Tests Passed:
```
âœ“ Current Stage: Grade 0: OVERFIT Single Spawner
âœ“ Spawner Health Mult: 0.15 (15 HP = 0.15 Ã— 100)
âœ“ Shaping Scale: 15.0Ã— (MAXIMUM GUIDANCE!)
âœ“ Max Enemies: 0.0 (NO ENEMIES)
âœ“ Fixed Position: [(800, 640)] (CENTER)
âœ“ Max Steps: 5000 (more time to learn)

âœ“ Spawner Count: 1
âœ“ Spawner Position: (800.0, 640.0) âœ“
âœ“ Spawner Health: 15 / 100 âœ“
âœ“ Enemy Count: 0 (NO ENEMIES) âœ“

âœ“ Spawner Kill Reward: 400.0 (10Ã— BOOSTED!)
```

---

## ðŸš€ Training Commands

### Start Fresh Training (Grade 0):
```bash
# Style 1 (rotation + thrust - RECOMMENDED)
python -m arena.train --algo ppo --style 1 --steps 10000000

# Style 2 (fixed nozzle - harder)
python -m arena.train --algo ppo --style 2 --steps 10000000

# Monitor training
tensorboard --logdir ./runs
```

### Expected Console Output:
```
Episode 1000  | Reward: -15.2 | Kills: 0.05 | Stage: Grade 0
Episode 5000  | Reward: -12.8 | Kills: 0.15 | Stage: Grade 0
Episode 10000 | Reward: -8.5  | Kills: 0.35 | Stage: Grade 0  â† Approaching!
Episode 15000 | Reward: +50.2 | Kills: 0.65 | Stage: Grade 0  â† First kills!
Episode 25000 | Reward: +280  | Kills: 0.82 | Stage: Grade 0  â† Mastered!
Advanced to stage: 0 â†’ 1 (Grade 1: Transfer to Random Positions)
```

---

## ðŸ“Š Metrics to Monitor

### TensorBoard:
1. **rollout/ep_rew_mean**: Should increase from -15 â†’ +300
2. **curriculum/spawner_kills**: Should increase 0 â†’ 0.8+
3. **curriculum/damage_dealt**: Should increase 0 â†’ 15+
4. **curriculum/stage_index**: Should advance 0 â†’ 1 after ~1-2M steps
5. **train/entropy**: Should decrease (policy becoming deterministic)

### Success Indicators:
- âœ… **200K steps**: Approaching spawner consistently
- âœ… **500K steps**: Shooting near spawner
- âœ… **800K steps**: First spawner damage
- âœ… **1.5M steps**: First spawner kill
- âœ… **2M steps**: 80%+ kill rate â†’ **Advance to Grade 1!**

---

## ðŸ”§ Troubleshooting

### If stuck in Grade 0 (>3M steps, <50% kill rate):

#### Option 1: Make it even easier
```python
# In curriculum.py Grade 0
spawner_health_mult=0.10,        # 10 HP (1 shot!)
min_spawner_kill_rate=0.6,       # Lower requirement
shaping_scale_mult=20.0,         # Even stronger guidance!
```

#### Option 2: Increase reward magnitude
```python
# In config.py
REWARD_SPAWNER_DESTROYED = 1000.0  # Was 400
REWARD_HIT_SPAWNER = 20.0          # Was 6.0
```

#### Option 3: Add demonstration learning
```bash
# Record 5-10 human demos
python -m arena.train --record-demo --human --episodes 10

# Train with behavioral cloning
python -m arena.train --algo bc --demo-dir ./demos --epochs 50

# Fine-tune with RL
python -m arena.train --algo ppo --load-bc-model ./models/bc_final.zip
```

---

## ðŸŽ“ Why This Works (Theory)

### 1. **Sparse Reward â†’ Dense Reward**
- **Problem**: Spawner kill is rare event (sparse reward)
- **Solution**: Dense shaping rewards guide toward sparse reward
- **Result**: Agent learns incrementally (approach â†’ aim â†’ shoot â†’ kill)

### 2. **Curriculum Scaffolding**
- **Problem**: Full task too complex (multiple spawners, enemies, phases)
- **Solution**: Master atomic task first (single spawner, no enemies)
- **Result**: Build competence layer-by-layer

### 3. **Overfitting as Feature**
- **Problem**: Need to find ANY winning strategy first
- **Solution**: Allow agent to **completely overfit** to fixed scenario
- **Result**: Fast convergence to ONE solution, then transfer

### 4. **Grokking Through Repetition**
- **Problem**: Agent needs to deeply understand game mechanics
- **Solution**: 200+ episodes on identical scenario
- **Result**: Agent "groks" the physics/rules â†’ sudden generalization

### 5. **Behavioral Cloning via Rewards**
- **Problem**: Hard to discover rare behaviors (aimed shooting)
- **Solution**: Reward the BEHAVIOR (shooting while aimed), not just outcome
- **Result**: Agent explores correct actions more frequently

---

## ðŸ”¬ Comparison to Previous System

| Metric | Old System (No Grade 0) | **New System (With Grade 0)** |
|--------|------------------------|-------------------------------|
| **First spawner kill** | Never (>4M steps) | **800K-1.5M steps** âœ“ |
| **80% kill rate** | Never | **1.5M-2M steps** âœ“ |
| **Spawner kill reward** | +40 | **+400 (10Ã—)** âœ“ |
| **Shaping rewards** | 5Ã— (Grade 1) | **15Ã— (Grade 0)** âœ“ |
| **Training variance** | Random positions | **Fixed position** âœ“ |
| **Spawner HP (Grade 1)** | 30 HP (3 shots) | **15 HP (1.5 shots!)** âœ“ |
| **Episode length** | 3000 steps | **5000 steps** âœ“ |
| **Curriculum stages** | 8 | **9 (added Grade 0)** âœ“ |

**Expected Improvement**: **5-10Ã— faster** to first meaningful behaviors!

---

## ðŸŽ¯ Next Steps After Grade 0

### When agent advances to Grade 1:
1. **Verify transfer**: Check if kill rate drops significantly
2. **If transfer fails** (<20% kill rate in Grade 1):
   - Add intermediate stage: 2-3 random positions to choose from
   - Or train longer in Grade 0 (lower advancement requirement)
3. **If transfer succeeds**: Continue through Grades 2-8 normally

### Future Enhancements (Not Yet Implemented):
- **Replay buffer overfitting**: Oversample successful episodes 10:1
- **State archive**: Save states where agent killed spawner, reset from them
- **Demonstration bootstrapping**: Train BC model on human demos first
- **Partial damage rewards**: Reward at 75%, 50%, 25% spawner HP thresholds
- **Curiosity-driven exploration**: Add intrinsic motivation bonus

---

## ðŸ“ Files Modified

### Core Changes:
1. **`arena/core/curriculum.py`**:
   - Added `fixed_spawner_positions` parameter to `CurriculumStage`
   - Added `max_episode_steps` parameter to `CurriculumStage`
   - Added **Grade 0: OVERFIT Single Spawner** as first stage
   - Shifted all existing grades (Grade 1 â†’ Grade 2, etc.)

2. **`arena/core/config.py`**:
   - Changed `REWARD_SPAWNER_DESTROYED` from 40 â†’ **400** (10Ã— boost)

3. **`arena/core/environment.py`**:
   - Modified `_init_phase()` to use `fixed_spawner_positions` if provided
   - Modified step limit check to use `max_episode_steps` if provided

### Test & Documentation:
4. **`test_overfit_curriculum.py`**: Validation test script (âœ… all pass)
5. **`OVERFIT_GROKKING_GUIDE.md`**: This comprehensive guide

---

## ðŸŽ‰ Summary

### What You Get:
âœ… **Grade 0** - Simplest possible task (fixed spawner, no enemies)  
âœ… **15 HP spawner** - Only 1.5 shots to kill!  
âœ… **15Ã— shaping** - Maximum guidance rewards  
âœ… **+400 kill reward** - 10Ã— normal value  
âœ… **5000 step episodes** - More exploration time  
âœ… **Fixed position** - Complete overfitting allowed  
âœ… **Validated** - All tests pass  
âœ… **Documented** - Full guide and troubleshooting  

### Expected Result:
**Agent will learn to kill spawners within 1-2M steps** (vs never learning before!)

### Training Command:
```bash
python -m arena.train --algo ppo --style 1 --steps 10000000
tensorboard --logdir ./runs
```

### Success Criteria:
- **1.5M steps**: First spawner kills
- **2M steps**: 80%+ kill rate in Grade 0
- **2.5M steps**: Advance to Grade 1
- **5M steps**: Consistent performance in Grade 1-2
- **10M steps**: First wins in Grade 5-6

ðŸš€ **Ready to train! The agent will finally learn to destroy spawners!**
