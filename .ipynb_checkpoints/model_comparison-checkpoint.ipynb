{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Notebook\n",
    "\n",
    "This notebook reads and analyzes model evaluation results from CSV files, providing comprehensive visualizations and comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports and Setup\n",
    "# =========================\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If you want nicer defaults (optional)\n",
    "plt.rcParams[\"figure.dpi\"] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Load CSV\n",
    "# -----------\n",
    "# Option A: read from file\n",
    "CSV_PATH = \"comparison_s1.csv\"  # <-- change me\n",
    "df_raw = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Option B: if you have the CSV text in a cell/string, you can do:\n",
    "# from io import StringIO\n",
    "# df_raw = pd.read_csv(StringIO(CSV_TEXT))\n",
    "\n",
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Cleaning / parsing helpers\n",
    "# -------------------------\n",
    "NA_LIKE = {\"N/A\", \"NA\", \"nan\", \"NaN\", \"\", None}\n",
    "\n",
    "def parse_mean_pm(s: str):\n",
    "    \"\"\"\n",
    "    Parse strings like \"-114.86 ± 9.00\" into (mean, std).\n",
    "    Returns (np.nan, np.nan) if not parseable.\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return (np.nan, np.nan)\n",
    "    s = str(s).strip()\n",
    "    if s in NA_LIKE:\n",
    "        return (np.nan, np.nan)\n",
    "    m = re.match(r\"^\\s*([+-]?\\d+(\\.\\d+)?)\\s*(?:±|\\+/-)\\s*([+-]?\\d+(\\.\\d+)?)\\s*$\", s)\n",
    "    if not m:\n",
    "        return (np.nan, np.nan)\n",
    "    return (float(m.group(1)), float(m.group(3)))\n",
    "\n",
    "def to_float(s):\n",
    "    \"\"\"Convert to float, tolerating N/A and stray whitespace.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s).strip()\n",
    "    if s in NA_LIKE:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify \"mean ± std\" columns by scanning for the symbol\n",
    "pm_cols = [c for c in df.columns if df[c].astype(str).str.contains(\"±\", na=False).any()]\n",
    "\n",
    "# Parse mean±std columns into two numeric columns: <col>_mean and <col>_std\n",
    "for c in pm_cols:\n",
    "    means, stds = zip(*df[c].map(parse_mean_pm))\n",
    "    df[c + \"_mean\"] = pd.to_numeric(means, errors=\"coerce\")\n",
    "    df[c + \"_std\"]  = pd.to_numeric(stds, errors=\"coerce\")\n",
    "\n",
    "# Convert obvious numeric columns (including percent columns) to floats when possible\n",
    "# We'll avoid touching \"Model\" and other known categorical columns.\n",
    "categorical_cols = {\"Model\", \"Algorithm\"}\n",
    "for c in df.columns:\n",
    "    if c in categorical_cols:\n",
    "        continue\n",
    "    # Skip the original mean±std string cols; we already parsed them\n",
    "    if c in pm_cols:\n",
    "        continue\n",
    "    # If a column looks numeric-ish, convert\n",
    "    if df[c].dtype == object:\n",
    "        # try conversion; if enough values convert, keep it\n",
    "        converted = df[c].map(to_float)\n",
    "        # heuristic: if at least 30% parse, treat as numeric\n",
    "        if converted.notna().mean() >= 0.3:\n",
    "            df[c] = converted\n",
    "\n",
    "# Make sure some key fields exist as numeric\n",
    "for c in [\"Style\", \"Deterministic\", \"Win Rate (%)\", \"Speed (eps/sec)\"]:\n",
    "    if c in df.columns and df[c].dtype == object:\n",
    "        df[c] = df[c].map(to_float)\n",
    "\n",
    "# Deterministic: keep as bool-ish if it is True/False strings\n",
    "if \"Deterministic\" in df.columns:\n",
    "    if df[\"Deterministic\"].dtype == object:\n",
    "        df[\"Deterministic\"] = df[\"Deterministic\"].astype(str).str.strip().map(\n",
    "            {\"True\": True, \"False\": False, \"true\": True, \"false\": False}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Convenience columns\n",
    "# -------------------------\n",
    "# A shorter label for plotting\n",
    "df[\"ModelShort\"] = df[\"Model\"].astype(str).str.replace(\"_final.zip\", \"\", regex=False)\n",
    "\n",
    "# Reward mean/std columns (if your CSV uses \"Mean Reward\" with ±)\n",
    "if \"Mean Reward_mean\" in df.columns:\n",
    "    df[\"MeanReward\"] = df[\"Mean Reward_mean\"]\n",
    "    df[\"StdReward\"] = df[\"Mean Reward_std\"]\n",
    "elif \"Mean Reward\" in df.columns and pd.api.types.is_numeric_dtype(df[\"Mean Reward\"]):\n",
    "    df[\"MeanReward\"] = df[\"Mean Reward\"]\n",
    "    df[\"StdReward\"] = np.nan\n",
    "\n",
    "# Episode length mean/std (if present as ±)\n",
    "if \"Mean Episode Length_mean\" in df.columns:\n",
    "    df[\"MeanEpLen\"] = df[\"Mean Episode Length_mean\"]\n",
    "    df[\"StdEpLen\"] = df[\"Mean Episode Length_std\"]\n",
    "elif \"Mean Episode Length\" in df.columns and pd.api.types.is_numeric_dtype(df[\"Mean Episode Length\"]):\n",
    "    df[\"MeanEpLen\"] = df[\"Mean Episode Length\"]\n",
    "    df[\"StdEpLen\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Filtering / grouping knobs\n",
    "# -------------------------\n",
    "# Edit these lists to focus comparisons\n",
    "FILTER_ALGORITHMS = None     # e.g. [\"PPO\", \"PPO_LSTM\"] or None for all\n",
    "FILTER_STYLE      = None     # e.g. [2]\n",
    "FILTER_DETERMIN   = None     # e.g. [True] or [False] or None\n",
    "\n",
    "df_plot = df.copy()\n",
    "if FILTER_ALGORITHMS is not None and \"Algorithm\" in df_plot.columns:\n",
    "    df_plot = df_plot[df_plot[\"Algorithm\"].isin(FILTER_ALGORITHMS)]\n",
    "if FILTER_STYLE is not None and \"Style\" in df_plot.columns:\n",
    "    df_plot = df_plot[df_plot[\"Style\"].isin(FILTER_STYLE)]\n",
    "if FILTER_DETERMIN is not None and \"Deterministic\" in df_plot.columns:\n",
    "    df_plot = df_plot[df_plot[\"Deterministic\"].isin(FILTER_DETERMIN)]\n",
    "\n",
    "# Optional: choose sort key for model ordering in plots\n",
    "SORT_BY = \"MeanReward\"  # e.g. \"Win Rate (%)\", \"Speed (eps/sec)\", \"MeanReward\", \"MeanEpLen\"\n",
    "if SORT_BY in df_plot.columns:\n",
    "    df_plot = df_plot.sort_values(SORT_BY, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Plot helpers\n",
    "# -------------------------\n",
    "def barh_with_optional_err(x, labels, xerr=None, title=\"\", xlabel=\"\"):\n",
    "    plt.figure(figsize=(10, max(3, 0.35 * len(labels))))\n",
    "    y = np.arange(len(labels))\n",
    "    if xerr is not None and np.isfinite(xerr).any():\n",
    "        plt.barh(y, x, xerr=xerr, capsize=3)\n",
    "    else:\n",
    "        plt.barh(y, x)\n",
    "    plt.yticks(y, labels)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def safe_col(name):\n",
    "    return name in df_plot.columns and pd.api.types.is_numeric_dtype(df_plot[name])\n",
    "\n",
    "labels = df_plot[\"ModelShort\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Comparison Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Win Rate\n",
    "if safe_col(\"Win Rate (%)\"):\n",
    "    barh_with_optional_err(\n",
    "        df_plot[\"Win Rate (%)\"].to_numpy(),\n",
    "        labels,\n",
    "        title=\"Win Rate by Model\",\n",
    "        xlabel=\"Win Rate (%)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed\n",
    "if safe_col(\"Speed (eps/sec)\"):\n",
    "    barh_with_optional_err(\n",
    "        df_plot[\"Speed (eps/sec)\"].to_numpy(),\n",
    "        labels,\n",
    "        title=\"Evaluation Speed by Model\",\n",
    "        xlabel=\"Episodes per second (eps/sec)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Reward\n",
    "if safe_col(\"MeanReward\"):\n",
    "    xerr = df_plot[\"StdReward\"].to_numpy() if safe_col(\"StdReward\") else None\n",
    "    barh_with_optional_err(\n",
    "        df_plot[\"MeanReward\"].to_numpy(),\n",
    "        labels,\n",
    "        xerr=xerr,\n",
    "        title=\"Mean Reward by Model (± std if available)\",\n",
    "        xlabel=\"Mean Reward\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Episode Length\n",
    "if safe_col(\"MeanEpLen\"):\n",
    "    xerr = df_plot[\"StdEpLen\"].to_numpy() if safe_col(\"StdEpLen\") else None\n",
    "    barh_with_optional_err(\n",
    "        df_plot[\"MeanEpLen\"].to_numpy(),\n",
    "        labels,\n",
    "        xerr=xerr,\n",
    "        title=\"Mean Episode Length by Model (± std if available)\",\n",
    "        xlabel=\"Mean Episode Length (steps)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional metrics\n",
    "for extra, title, xlabel in [\n",
    "    (\"Avg Phase Reached\", \"Average Phase Reached by Model\", \"Avg Phase Reached\"),\n",
    "    (\"Avg Final HP\", \"Average Final HP by Model\", \"Avg Final HP\"),\n",
    "    (\"First Spawner Kill (steps)\", \"First Spawner Kill (steps) by Model\", \"Steps\"),\n",
    "]:\n",
    "    if safe_col(extra):\n",
    "        barh_with_optional_err(df_plot[extra].to_numpy(), labels, title=title, xlabel=xlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Phase distribution heatmap (Phase 1..5)\n",
    "# -------------------------\n",
    "phase_cols = [c for c in df_plot.columns if re.fullmatch(r\"Phase [1-5] \\(%\\)\", c)]\n",
    "phase_cols = sorted(phase_cols, key=lambda x: int(re.findall(r\"\\d+\", x)[0]))  # ensure order\n",
    "\n",
    "if len(phase_cols) > 0 and all(safe_col(c) for c in phase_cols):\n",
    "    phase_mat = df_plot[phase_cols].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, max(3, 0.35 * len(labels))))\n",
    "    plt.imshow(phase_mat, aspect=\"auto\")\n",
    "    plt.yticks(np.arange(len(labels)), labels)\n",
    "    plt.xticks(np.arange(len(phase_cols)), [c.replace(\" (%)\", \"\") for c in phase_cols], rotation=0)\n",
    "    plt.title(\"Phase Reach Distribution (%) by Model\")\n",
    "    plt.colorbar(label=\"%\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Spawners killed distribution heatmap (0..8)\n",
    "# -------------------------\n",
    "spawner_cols = [c for c in df_plot.columns if re.fullmatch(r\"\\d+ Spawners Killed \\(%\\)\", c)]\n",
    "# Sort numerically by leading number\n",
    "spawner_cols = sorted(spawner_cols, key=lambda x: int(re.match(r\"(\\d+)\", x).group(1))) if spawner_cols else []\n",
    "\n",
    "if len(spawner_cols) > 0 and all(safe_col(c) for c in spawner_cols):\n",
    "    sp_mat = df_plot[spawner_cols].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, max(3, 0.35 * len(labels))))\n",
    "    plt.imshow(sp_mat, aspect=\"auto\")\n",
    "    plt.yticks(np.arange(len(labels)), labels)\n",
    "    plt.xticks(np.arange(len(spawner_cols)), [re.match(r\"(\\d+)\", c).group(1) for c in spawner_cols])\n",
    "    plt.title(\"Spawners Killed Distribution (%) by Model\")\n",
    "    plt.xlabel(\"# Spawners killed\")\n",
    "    plt.colorbar(label=\"%\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Scatter: Speed vs Reward (and win rate as point size if available)\n",
    "# -------------------------\n",
    "if safe_col(\"Speed (eps/sec)\") and safe_col(\"MeanReward\"):\n",
    "    x = df_plot[\"Speed (eps/sec)\"].to_numpy()\n",
    "    y = df_plot[\"MeanReward\"].to_numpy()\n",
    "\n",
    "    sizes = None\n",
    "    if safe_col(\"Win Rate (%)\"):\n",
    "        wr = df_plot[\"Win Rate (%)\"].to_numpy()\n",
    "        # scale sizes gently (avoid 0)\n",
    "        sizes = 30 + 3 * np.nan_to_num(wr, nan=0.0)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(x, y, s=sizes)\n",
    "    for i, name in enumerate(labels):\n",
    "        plt.annotate(name, (x[i], y[i]), fontsize=8, xytext=(4, 2), textcoords=\"offset points\")\n",
    "    plt.title(\"Speed vs Mean Reward\")\n",
    "    plt.xlabel(\"Speed (eps/sec)\")\n",
    "    plt.ylabel(\"Mean Reward\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Correlation heatmap of key metrics\n",
    "# -------------------------\n",
    "key_metrics = [\n",
    "    \"Win Rate (%)\",\n",
    "    \"Speed (eps/sec)\",\n",
    "    \"MeanReward\",\n",
    "    \"MeanEpLen\",\n",
    "    \"Avg Phase Reached\",\n",
    "    \"Avg Final HP\",\n",
    "    \"First Spawner Kill (steps)\",\n",
    "]\n",
    "key_metrics = [c for c in key_metrics if safe_col(c)]\n",
    "\n",
    "if len(key_metrics) >= 3:\n",
    "    corr = df_plot[key_metrics].corr(numeric_only=True).to_numpy()\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.imshow(corr, aspect=\"auto\")\n",
    "    plt.xticks(np.arange(len(key_metrics)), key_metrics, rotation=45, ha=\"right\")\n",
    "    plt.yticks(np.arange(len(key_metrics)), key_metrics)\n",
    "    plt.title(\"Correlation (key metrics)\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Quick table to sanity check parsing\n",
    "# -------------------------\n",
    "display_cols = [\"ModelShort\", \"Algorithm\", \"Style\", \"Deterministic\"]\n",
    "for c in [\"Episodes\", \"Eval Time (s)\", \"Speed (eps/sec)\", \"Win Rate (%)\", \"MeanReward\", \"StdReward\", \"MeanEpLen\", \"StdEpLen\"]:\n",
    "    if c in df_plot.columns:\n",
    "        display_cols.append(c)\n",
    "\n",
    "print(\"Rows:\", len(df_plot))\n",
    "display(df_plot[display_cols].reset_index(drop=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
